{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "My9EfdVJBsFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fe1c2f-f7ac-41ef-a459-2e9ee36478a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VOOipcFTGNpa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "metadata": {
        "id": "lC8Yo0JqYGSj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "import random\n"
      ],
      "metadata": {
        "id": "duJyJfNW2tgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3785cc-24e4-44ff-94e5-c0ca2339bf54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "path_to_data = \"/gdrive/MyDrive/MIE1666project/rl-ccmvo-main/rl-ccmvo-main/Data/synthetic1000\"\n",
        "date = \"2016-02-07\"\n",
        "mu = np.loadtxt('{}/{}/mu'.format(path_to_data, date))\n",
        "Q = np.loadtxt('{}/{}/Q'.format(path_to_data, date))"
      ],
      "metadata": {
        "id": "Yx4vejQpGZ4k"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization of Q and mu"
      ],
      "metadata": {
        "id": "Nict7JmGOtn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "Q/=torch.linalg.norm(torch.tensor(Q))\n",
        "mu/=torch.linalg.norm(torch.tensor(mu))"
      ],
      "metadata": {
        "id": "T2uxqpGOj_mp"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three important function for the RL env set-up, **MVO(mu,Q,gamma), generate_Q(Q,index) and derivative(mu,Q,gamma,w)**\n",
        "\n",
        "\n",
        "*   MVO(mu,Q,gamma) returns the weight vector and the negative MVO (return-risk) value.\n",
        "*   generate_Q(Q,index) is used in each time step for the agent to generate a **Q** convariance for allocated k assets.\n",
        "*   derivative(mu,Q,gamma,w) is used to give the derivative MVO which is used to provide the reward flag for +1 if derivative >0 or else reward=-1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G33GzoDsOy2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cvxpy as cp\n",
        "def MVO(mu,Q,gamma):\n",
        "    # Convert mu to NumPy array if it's not already\n",
        "  if not isinstance(mu, np.ndarray):\n",
        "      mu = np.array(mu)\n",
        "\n",
        "  # Convert Q to NumPy array if it's not already\n",
        "  if not isinstance(Q, np.ndarray):\n",
        "      Q = np.array(Q)\n",
        "  if mu.size == 0 or Q.size == 0:\n",
        "    return 0,0\n",
        "  else:\n",
        "    n = len(mu)\n",
        "\n",
        "    # Decision Variables\n",
        "    w = cp.Variable(n)\n",
        "\n",
        "    constraints = [\n",
        "        cp.sum(w) == 1, # Sum to 1\n",
        "        w>=0 # Disallow Short Sales\n",
        "    ]\n",
        "\n",
        "    # Objective Function\n",
        "    risk = cp.quad_form(w, Q)\n",
        "    targetRet=gamma*mu.T@w\n",
        "    prob = cp.Problem(cp.Maximize(targetRet - risk), constraints=constraints)\n",
        "    prob.solve()\n",
        "    return w.value,targetRet.value-risk.value"
      ],
      "metadata": {
        "id": "06AJ65k7E145"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to select Qs from the Q array of 1000/500 el\n",
        "# def generate_Q(Q,index):\n",
        "#   # Use np.meshgrid to generate all pairs of indices\n",
        "#   all_pairs = np.array(np.meshgrid(index,index)).T.reshape(-1, 2)\n",
        "\n",
        "#   # Use advanced indexing to select the elements\n",
        "#   selected_elements = Q[all_pairs[:, 0], all_pairs[:, 1]]\n",
        "#   size=int(np.sqrt(selected_elements.size))\n",
        "#   return selected_elements.reshape(size,size)\n",
        "def generate_Q(Q, index):\n",
        "    # Convert index to a PyTorch tensor\n",
        "    index_tensor = torch.tensor(index)\n",
        "\n",
        "    # Use torch.meshgrid to generate all pairs of indices\n",
        "    all_pairs = torch.meshgrid(index_tensor, index_tensor)\n",
        "    all_pairs = torch.stack(all_pairs, dim=-1).reshape(-1, 2)\n",
        "\n",
        "    # Use advanced indexing to select the elements\n",
        "    selected_elements = Q[all_pairs[:, 0], all_pairs[:, 1]]\n",
        "\n",
        "    # Calculate the size based on the length of the index array\n",
        "    size = len(index)\n",
        "\n",
        "    return selected_elements.reshape(size, size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqIvXV4_3Aac",
        "outputId": "8ff88016-d4b4-4046-e0c3-653dd9b23988"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# derivative function always return the gradient of MVO wrt the last x added to the list\n",
        "def derivative(mu,Q,gamma,w):\n",
        "  # if mu.size <= 1 or Q.size <= 1:\n",
        "  #   return -gamma*mu[0]\n",
        "  # else:\n",
        "  risk=0\n",
        "  for i in range(len(mu)):\n",
        "      risk+=Q[i][-1]*w[i]\n",
        "  Ret=mu[-1]*gamma\n",
        "      # Ret+=mu[-1]*gamma\n",
        "  return Ret-risk\n"
      ],
      "metadata": {
        "id": "MAXDLo-VQr77"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build Our Environment"
      ],
      "metadata": {
        "id": "f0fFm5QcSC6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainEnv(Env):\n",
        "## initialize parameter:##\n",
        "\n",
        "    def __init__(self, Q):\n",
        "\n",
        "        self.index = [] #index of the latest asset added to allocation\n",
        "        self.cur_mu = [] #cur_mu and cur_Q at time step k\n",
        "        self.cur_Q = []\n",
        "        self.action_space = Discrete(500) # action_space and observation _space are defined, action space should be 1000, e.g 0 means the first asset is selected\n",
        "        self.observation_space = Box(low=-np.inf, high=np.inf)\n",
        "        # state is a numpy array float\n",
        "        self.state =np.array([0.]) # Initialize state which should be state[0]= MVO if updated\n",
        "        self.asset_length = 50 # asset length to end\n",
        "        self.used_actions = set()  # Keep track of used actions in the current iteration\n",
        "\n",
        "    def step(self, action, done):\n",
        "        if done == True:\n",
        "          self.used_actions=set()\n",
        "          self.index = [] #index of the latest asset added to allocation\n",
        "          self.cur_mu = [] #cur_mu and cur_Q at time step k\n",
        "          self.cur_Q = []\n",
        "        # action is the index of the asset\n",
        "        action = self.sample_unique_action()\n",
        "        self.used_actions.add(action)\n",
        "        gamma = 1\n",
        "        self.index.append(action)\n",
        "        self.cur_mu.append(mu[action])\n",
        "        self.cur_Q=generate_Q(Q,self.index) # Assuming generate_Q is defined\n",
        "        # print(\"cur mu:\",len(self.cur_mu),\"cur_Q\",self.cur_Q.size())\n",
        "        w, self.state[0] = MVO(self.cur_mu, self.cur_Q, gamma)  # Assuming MVO is defined\n",
        "        self.asset_length -= 1\n",
        "        self.transition = derivative(self.cur_mu,self.cur_Q, gamma, w)  # Assuming derivative is defined\n",
        "\n",
        "        # Calculate reward\n",
        "        reward=self.transition.item()\n",
        "        # if self.transition > 0:\n",
        "        #     reward = 1\n",
        "        # else:\n",
        "        #     reward = -1\n",
        "\n",
        "        done = self.asset_length <= 0\n",
        "\n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "\n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    # Make sure each action are unique\n",
        "    def sample_unique_action(self):\n",
        "      # Sample a new action until it is unique in the current iteration\n",
        "      new_action = self.action_space.sample()\n",
        "      while new_action in self.used_actions:\n",
        "          new_action = self.action_space.sample()\n",
        "      return new_action\n",
        "    # def render(self):\n",
        "    #     # Implement viz\n",
        "    #     pass\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset\n",
        "        self.state = np.array([0.])\n",
        "        self.used_actions.clear()\n",
        "        self.asset_length = 50\n",
        "        return self.state\n"
      ],
      "metadata": {
        "id": "yuKH-Foc20b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3729cc6-73a3-4b1b-ac13-691a3a311c40"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=TrainEnv(Q)"
      ],
      "metadata": {
        "id": "v4BLgF1oXp2i"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h7gtwLntvUL",
        "outputId": "1aaad91b-8fb4-48c5-e46a-99add0204839"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action=env.action_space.sample()\n",
        "action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4Rup6E4LR2M",
        "outputId": "72c56538-347a-4d85-a57c-6fd8559488b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "458"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN model (not finished)\n",
        "We will need two Neural network in DQN, one is for evaluation and one is for forward pass (Target)."
      ],
      "metadata": {
        "id": "oqHUToEjSKQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper Parameters\n",
        "BATCH_SIZE = 32\n",
        "LR = 0.01                   # learning rate\n",
        "EPSILON = 0.8               # greedy policy\n",
        "GAMMA = 0.9                 # reward discount\n",
        "TARGET_REPLACE_ITER = 100   # target update frequency\n",
        "MEMORY_CAPACITY = 2000 # Memory for experience replay\n",
        "# env = gym.make('CartPole-v0')\n",
        "# env = env.unwrapped\n",
        "N_ACTIONS = env.action_space.n\n",
        "N_STATES = env.observation_space.shape[0]\n",
        "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape"
      ],
      "metadata": {
        "id": "Wfx-4AVuegDN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_ACTIONS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfvaMDSomBjx",
        "outputId": "06a40ffa-9f1d-48c3-95ec-fb8297852f57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(N_STATES, 10)\n",
        "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
        "        self.out = nn.Linear(10, N_ACTIONS)\n",
        "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        actions_value = self.out(x)\n",
        "        return actions_value"
      ],
      "metadata": {
        "id": "NT6LyzDeXoj0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_net=Net()\n",
        "# x=env.reset()\n",
        "# x= torch.unsqueeze(torch.FloatTensor(x), 0)\n",
        "# actions_value = eval_net.forward(x)\n",
        "# for i in used_action:\n",
        "#     actions_value[0][i] = -10000\n",
        "# action = torch.max(actions_value, 1)[1].data.numpy()\n",
        "# action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
        "# used_action.add(action)\n",
        "# torch.max(actions_value, 1)[1].data.numpy()"
      ],
      "metadata": {
        "id": "sd2kAoR7ogRT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(object):\n",
        "    def __init__(self):\n",
        "        self.eval_net, self.target_net = Net(), Net()\n",
        "        self.learn_step_counter = 0                                     # for target updating\n",
        "        self.memory_counter = 0                                         # for storing memory\n",
        "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
        "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
        "        self.loss_func = nn.MSELoss()\n",
        "        self.used_actions = set()  # Keep track of used actions in the current iteration\n",
        "\n",
        "\n",
        "    def choose_action(self,x,done):\n",
        "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
        "        # input only one sample\n",
        "        if done == True:\n",
        "          self.used_actions=set()\n",
        "        if np.random.uniform() < EPSILON:   # greedy\n",
        "            # print(\"it's greedy\")\n",
        "            actions_value = self.eval_net.forward(x)\n",
        "            for i in self.used_actions:\n",
        "                actions_value[0][i] = -1000 #assign a large negative value for selected action such that it wont select it again\n",
        "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
        "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
        "\n",
        "        else:   # random\n",
        "            # print(\"it's random\")\n",
        "            available_actions = list(set(range(N_ACTIONS))- self.used_actions)\n",
        "            action = np.random.choice(available_actions)\n",
        "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
        "        self.used_actions.add(action)\n",
        "\n",
        "\n",
        "        # print(\"used action:\",self.used_actions)\n",
        "        # print(\"Used action:\",self.used_actions,\"action for this iter:\",action, \"ENV_A_SHAPE:\",ENV_A_SHAPE)\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, s, a, r, s_):\n",
        "        transition = np.hstack((s, [a, r], s_))\n",
        "        # replace the old memory with new memory\n",
        "        index = self.memory_counter % MEMORY_CAPACITY\n",
        "        self.memory[index, :] = transition\n",
        "        self.memory_counter += 1\n",
        "    def learn(self):\n",
        "        # target parameter update\n",
        "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
        "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
        "        self.learn_step_counter += 1\n",
        "\n",
        "        # sample batch transitions\n",
        "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
        "        b_memory = self.memory[sample_index, :]\n",
        "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
        "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
        "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
        "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
        "\n",
        "        # q_eval w.r.t the action in experience\n",
        "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
        "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
        "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
        "        loss = self.loss_func(q_eval, q_target)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "dODjGdSGXwbK"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN()"
      ],
      "metadata": {
        "id": "FJq8SYUjX1sz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN()\n",
        "done=False\n",
        "print('\\nCollecting experience...')\n",
        "for i_episode in range(200):\n",
        "    s = env.reset()\n",
        "    ep_r = 0\n",
        "    while True:\n",
        "        a = dqn.choose_action(s,done)\n",
        "        s_, r, done, info= env.step(a,done)\n",
        "        # if done ==True:\n",
        "        #   s=env.reset()\n",
        "        # modify the reward\n",
        "        # x, x_dot, theta, theta_dot = s_\n",
        "        # r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        # r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "        # r = r1 + r2\n",
        "        dqn.store_transition(s, a, r, s_)\n",
        "        ep_r += r\n",
        "        # print(ep_r,done,dqn.memory_counter)\n",
        "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
        "            # print('Learning')\n",
        "            dqn.learn()\n",
        "            if done:\n",
        "                print('Ep: ', i_episode,\n",
        "                      '| Ep_r: ', round(ep_r, 2))\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        s = s_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL-ALYddYVXT",
        "outputId": "95554c49-4ad9-4961-9b4b-4229d7a6c0af"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting experience...\n",
            "Ep:  40 | Ep_r:  1.1\n",
            "Ep:  41 | Ep_r:  1.23\n",
            "Ep:  42 | Ep_r:  1.19\n",
            "Ep:  43 | Ep_r:  1.32\n",
            "Ep:  44 | Ep_r:  1.33\n",
            "Ep:  45 | Ep_r:  1.43\n",
            "Ep:  46 | Ep_r:  1.36\n",
            "Ep:  47 | Ep_r:  1.13\n",
            "Ep:  48 | Ep_r:  1.14\n",
            "Ep:  49 | Ep_r:  1.06\n",
            "Ep:  50 | Ep_r:  1.23\n",
            "Ep:  51 | Ep_r:  1.38\n",
            "Ep:  52 | Ep_r:  1.23\n",
            "Ep:  53 | Ep_r:  1.06\n",
            "Ep:  54 | Ep_r:  1.11\n",
            "Ep:  55 | Ep_r:  1.19\n",
            "Ep:  56 | Ep_r:  1.18\n",
            "Ep:  57 | Ep_r:  1.15\n",
            "Ep:  58 | Ep_r:  1.09\n",
            "Ep:  59 | Ep_r:  1.26\n",
            "Ep:  60 | Ep_r:  1.14\n",
            "Ep:  61 | Ep_r:  1.37\n",
            "Ep:  62 | Ep_r:  1.44\n",
            "Ep:  63 | Ep_r:  1.1\n",
            "Ep:  64 | Ep_r:  0.93\n",
            "Ep:  65 | Ep_r:  1.03\n",
            "Ep:  66 | Ep_r:  1.47\n",
            "Ep:  67 | Ep_r:  1.09\n",
            "Ep:  68 | Ep_r:  1.31\n",
            "Ep:  69 | Ep_r:  1.16\n",
            "Ep:  70 | Ep_r:  1.15\n",
            "Ep:  71 | Ep_r:  1.26\n",
            "Ep:  72 | Ep_r:  1.29\n",
            "Ep:  73 | Ep_r:  1.23\n",
            "Ep:  74 | Ep_r:  1.31\n",
            "Ep:  75 | Ep_r:  1.36\n",
            "Ep:  76 | Ep_r:  1.1\n",
            "Ep:  77 | Ep_r:  1.13\n",
            "Ep:  78 | Ep_r:  1.39\n",
            "Ep:  79 | Ep_r:  1.43\n",
            "Ep:  80 | Ep_r:  1.17\n",
            "Ep:  81 | Ep_r:  1.41\n",
            "Ep:  82 | Ep_r:  1.17\n",
            "Ep:  83 | Ep_r:  1.26\n",
            "Ep:  84 | Ep_r:  1.3\n",
            "Ep:  85 | Ep_r:  1.03\n",
            "Ep:  86 | Ep_r:  0.97\n",
            "Ep:  87 | Ep_r:  1.18\n",
            "Ep:  88 | Ep_r:  1.22\n",
            "Ep:  89 | Ep_r:  1.37\n",
            "Ep:  90 | Ep_r:  0.78\n",
            "Ep:  91 | Ep_r:  1.14\n",
            "Ep:  92 | Ep_r:  1.43\n",
            "Ep:  93 | Ep_r:  1.22\n",
            "Ep:  94 | Ep_r:  1.4\n",
            "Ep:  95 | Ep_r:  1.05\n",
            "Ep:  96 | Ep_r:  0.97\n",
            "Ep:  97 | Ep_r:  1.08\n",
            "Ep:  98 | Ep_r:  1.12\n",
            "Ep:  99 | Ep_r:  1.15\n",
            "Ep:  100 | Ep_r:  1.08\n",
            "Ep:  101 | Ep_r:  1.16\n",
            "Ep:  102 | Ep_r:  1.21\n",
            "Ep:  103 | Ep_r:  1.18\n",
            "Ep:  104 | Ep_r:  1.08\n",
            "Ep:  105 | Ep_r:  1.18\n",
            "Ep:  106 | Ep_r:  1.31\n",
            "Ep:  107 | Ep_r:  1.37\n",
            "Ep:  108 | Ep_r:  1.33\n",
            "Ep:  109 | Ep_r:  1.07\n",
            "Ep:  110 | Ep_r:  1.13\n",
            "Ep:  111 | Ep_r:  1.43\n",
            "Ep:  112 | Ep_r:  1.13\n",
            "Ep:  113 | Ep_r:  1.29\n",
            "Ep:  114 | Ep_r:  1.31\n",
            "Ep:  115 | Ep_r:  1.1\n",
            "Ep:  116 | Ep_r:  1.13\n",
            "Ep:  117 | Ep_r:  1.2\n",
            "Ep:  118 | Ep_r:  1.04\n",
            "Ep:  119 | Ep_r:  1.43\n",
            "Ep:  120 | Ep_r:  1.35\n",
            "Ep:  121 | Ep_r:  1.29\n",
            "Ep:  122 | Ep_r:  1.3\n",
            "Ep:  123 | Ep_r:  1.32\n",
            "Ep:  124 | Ep_r:  1.04\n",
            "Ep:  125 | Ep_r:  1.28\n",
            "Ep:  126 | Ep_r:  1.34\n",
            "Ep:  127 | Ep_r:  1.15\n",
            "Ep:  128 | Ep_r:  1.12\n",
            "Ep:  129 | Ep_r:  1.4\n",
            "Ep:  130 | Ep_r:  1.17\n",
            "Ep:  131 | Ep_r:  1.18\n",
            "Ep:  132 | Ep_r:  1.12\n",
            "Ep:  133 | Ep_r:  1.32\n",
            "Ep:  134 | Ep_r:  1.35\n",
            "Ep:  135 | Ep_r:  1.25\n",
            "Ep:  136 | Ep_r:  1.28\n",
            "Ep:  137 | Ep_r:  1.47\n",
            "Ep:  138 | Ep_r:  1.37\n",
            "Ep:  139 | Ep_r:  1.11\n",
            "Ep:  140 | Ep_r:  1.21\n",
            "Ep:  141 | Ep_r:  1.27\n",
            "Ep:  142 | Ep_r:  1.29\n",
            "Ep:  143 | Ep_r:  1.28\n",
            "Ep:  144 | Ep_r:  1.25\n",
            "Ep:  145 | Ep_r:  1.0\n",
            "Ep:  146 | Ep_r:  1.31\n",
            "Ep:  147 | Ep_r:  1.14\n",
            "Ep:  148 | Ep_r:  0.84\n",
            "Ep:  149 | Ep_r:  1.07\n",
            "Ep:  150 | Ep_r:  1.31\n",
            "Ep:  151 | Ep_r:  1.4\n",
            "Ep:  152 | Ep_r:  1.09\n",
            "Ep:  153 | Ep_r:  1.44\n",
            "Ep:  154 | Ep_r:  1.1\n",
            "Ep:  155 | Ep_r:  1.32\n",
            "Ep:  156 | Ep_r:  1.36\n",
            "Ep:  157 | Ep_r:  1.21\n",
            "Ep:  158 | Ep_r:  1.05\n",
            "Ep:  159 | Ep_r:  1.31\n",
            "Ep:  160 | Ep_r:  1.49\n",
            "Ep:  161 | Ep_r:  0.93\n",
            "Ep:  162 | Ep_r:  1.28\n",
            "Ep:  163 | Ep_r:  1.26\n",
            "Ep:  164 | Ep_r:  1.22\n",
            "Ep:  165 | Ep_r:  1.1\n",
            "Ep:  166 | Ep_r:  1.14\n",
            "Ep:  167 | Ep_r:  1.54\n",
            "Ep:  168 | Ep_r:  1.25\n",
            "Ep:  169 | Ep_r:  1.2\n",
            "Ep:  170 | Ep_r:  1.34\n",
            "Ep:  171 | Ep_r:  1.26\n",
            "Ep:  172 | Ep_r:  0.95\n",
            "Ep:  173 | Ep_r:  1.11\n",
            "Ep:  174 | Ep_r:  1.18\n",
            "Ep:  175 | Ep_r:  1.24\n",
            "Ep:  176 | Ep_r:  1.06\n",
            "Ep:  177 | Ep_r:  1.33\n",
            "Ep:  178 | Ep_r:  1.01\n",
            "Ep:  179 | Ep_r:  1.44\n",
            "Ep:  180 | Ep_r:  1.43\n",
            "Ep:  181 | Ep_r:  1.23\n",
            "Ep:  182 | Ep_r:  1.33\n",
            "Ep:  183 | Ep_r:  1.26\n",
            "Ep:  184 | Ep_r:  1.27\n",
            "Ep:  185 | Ep_r:  1.42\n",
            "Ep:  186 | Ep_r:  1.05\n",
            "Ep:  187 | Ep_r:  1.21\n",
            "Ep:  188 | Ep_r:  1.23\n",
            "Ep:  189 | Ep_r:  1.05\n",
            "Ep:  190 | Ep_r:  1.45\n",
            "Ep:  191 | Ep_r:  1.17\n",
            "Ep:  192 | Ep_r:  1.27\n",
            "Ep:  193 | Ep_r:  1.02\n",
            "Ep:  194 | Ep_r:  1.26\n",
            "Ep:  195 | Ep_r:  1.07\n",
            "Ep:  196 | Ep_r:  1.04\n",
            "Ep:  197 | Ep_r:  0.87\n",
            "Ep:  198 | Ep_r:  1.01\n",
            "Ep:  199 | Ep_r:  1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dqn = DQN()\n",
        "\n",
        "print('\\nCollecting experience...')\n",
        "for i_episode in range(400):\n",
        "    s = env.reset()\n",
        "    ep_r = 0\n",
        "    while True:\n",
        "        a = dqn.choose_action(s)\n",
        "        s_, r, done, info= env.step(a)\n",
        "        print(a,s_,r,done,ep_r)\n",
        "        # modify the reward\n",
        "        # x, x_dot, theta, theta_dot = s_\n",
        "        # r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
        "        # r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
        "        # r = r1 + r2\n",
        "\n",
        "        dqn.store_transition(s, a, r, s_)\n",
        "\n",
        "        ep_r += r\n",
        "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
        "            dqn.learn()\n",
        "            if done:\n",
        "                print('Ep: ', i_episode,\n",
        "                      '| Ep_r: ', round(ep_r, 2))\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        s = s_"
      ],
      "metadata": {
        "id": "2_yleDY5X5Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# episodes = 50\n",
        "# for episode in range(1, episodes+1):\n",
        "#     state = env.reset()\n",
        "#     done = False\n",
        "#     score = 0\n",
        "\n",
        "#     while not done:\n",
        "#         #env.render()\n",
        "#         action = env.action_space.sample()\n",
        "#         n_state, reward, done, info= env.step(action)\n",
        "#         score+=reward\n",
        "#     print('Episode:{} Score:{}'.format(episode, score))"
      ],
      "metadata": {
        "id": "G4F5tdTuYfZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "8Xldc9-Hd0iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install \"ray[rllib]\" tensorflow torch"
      ],
      "metadata": {
        "id": "HKsa1WKmI_7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ray.rllib.algorithms.dqn.dqn import DQNConfig"
      ],
      "metadata": {
        "id": "lQYSyW9Dcbh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config = DQNConfig()\n",
        "# print(config.replay_buffer_config)\n",
        "# replay_config = config.replay_buffer_config.update(\n",
        "#     {\n",
        "#         \"capacity\": 60000,\n",
        "#         \"prioritized_replay_alpha\": 0.5,\n",
        "#         \"prioritized_replay_beta\": 0.5,\n",
        "#         \"prioritized_replay_eps\": 3e-6,\n",
        "#     }\n",
        "# )\n",
        "# config = config.training(replay_buffer_config=replay_config)\n",
        "# config = config.resources(num_gpus=1)\n",
        "# config = config.rollouts(num_rollout_workers=3)\n",
        "# algo = config.build()\n",
        "# algo.train()"
      ],
      "metadata": {
        "id": "a3u0nserdNJi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}